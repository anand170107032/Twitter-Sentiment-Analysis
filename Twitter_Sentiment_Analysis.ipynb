{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter_Sentiment_Analysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "d_z4RHDd9isg",
        "SwLRPddxHjUY",
        "TrLVX-8HHt1k",
        "OAjEysWAH3Rj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM-ik6UG4MX4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Twitter Sentiment Analysis\n",
        "**Problem Statement**\n",
        "\n",
        "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
        "\n",
        "Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, our objective is to predict the labels on the test dataset.\n",
        "\n",
        "**Dataset** \n",
        "[here](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5QM_r806JOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ignoring warnings\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPVU87YK3hUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing libraries\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "import string\n",
        "import re # for regular expressions\n",
        "import nltk # for text manipulation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr7Rsqv08E3T",
        "colab_type": "text"
      },
      "source": [
        "##Loading train/test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VipdeL2t6PDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train  = pd.read_csv('train_E6oV3lV.csv')\n",
        "test = pd.read_csv('test_tweets_anuFYb8.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU0Rq8Q58Nx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuAeXv068QcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oBaA1DK7iv7",
        "colab_type": "text"
      },
      "source": [
        "##Text PreProcessing and Cleaning\n",
        "**Data Inspection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS_8krvA7ZW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#few non racist/sexist tweets\n",
        "train[train['label'] == 0].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjVIGrVy8bqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#few racist/sexist tweets\n",
        "train[train['label'] == 1].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP8okyCA8jqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checking dimensions of the train and test dataset\n",
        "\n",
        "train.shape, test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-pqAfWN810C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#glimpse at label-distribution in the train dataset\n",
        "\n",
        "train[\"label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAk1zUfL9E8g",
        "colab_type": "text"
      },
      "source": [
        "In the train dataset, we have 2,242 (\\~7%) tweets labeled as racist or sexist, and 29,720 (~93%) tweets labeled as non racist/sexist. So, it is an imbalanced classification challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmyKBU8a9P4b",
        "colab_type": "text"
      },
      "source": [
        "Now we will check the distribution of length of the tweets, in terms of words, in both train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV8w05Kw8_p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length_train = train['tweet'].str.len()\n",
        "length_test = test['tweet'].str.len()\n",
        "\n",
        "plt.hist(length_train, bins=20, label=\"train_tweets\")\n",
        "plt.hist(length_test, bins=20, label=\"test_tweets\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A48-HMMu9e-W",
        "colab_type": "text"
      },
      "source": [
        "The tweet-length distribution is more or less the same in both train and test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_z4RHDd9isg",
        "colab_type": "text"
      },
      "source": [
        "##Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGZONwfX9Szd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi = train.append(test, ignore_index=True)\n",
        "combi.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO83TplU-KXB",
        "colab_type": "text"
      },
      "source": [
        "#Remove unwanted text patterns from the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrW-5Suk-JEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "        \n",
        "    return input_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtd5LUVu-ScW",
        "colab_type": "text"
      },
      "source": [
        "**1. Removing Twitter Handles (@user)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0An2_hnN9pQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \n",
        "combi.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbByh9Fe-aQH",
        "colab_type": "text"
      },
      "source": [
        "**2. Removing Punctuations, Numbers, and Special Characters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRW-8BQy-Yxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "combi.head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN6I3xL0-t2T",
        "colab_type": "text"
      },
      "source": [
        "**3. Removing Short Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfT2kyVm-frq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slg58GFg-yu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c7JQSdF-96x",
        "colab_type": "text"
      },
      "source": [
        "**4. Text Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lTKCfxvjaO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1pt58Of_M1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalizing the tokenized tweets\n",
        "from textblob import Word\n",
        "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) #lemmatizing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnhrdvO4CNcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkN35FL7Dax2",
        "colab_type": "text"
      },
      "source": [
        "#Visualization from Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38lMru-mDp2t",
        "colab_type": "text"
      },
      "source": [
        "**A) Understanding the common words used in the tweets: WordCloud**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWDAz_i3CQuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let’s visualize all the words our data using the wordcloud plot.\n",
        "all_words = ' '.join([text for text in combi['tidy_tweet']])\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoZb9Oz8EhPV",
        "colab_type": "text"
      },
      "source": [
        "We can see most of the words are positive or neutral. Words like love, great, friend, smile, good, life are the most frequent ones. It doesn’t give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes (racist/sexist or not) in our train data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnF0O7OeExOH",
        "colab_type": "text"
      },
      "source": [
        "**B) Words in non racist/sexist tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYFERQa_Ear7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sle2eJk_E6ua",
        "colab_type": "text"
      },
      "source": [
        "Most of the frequent words are compatible with the sentiment, i.e, non-racist/sexists tweets. Similarly, we will plot the word cloud for the other sentiment. Expect to see negative, racist, and sexist terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz_M8a7EFCiU",
        "colab_type": "text"
      },
      "source": [
        "**C) Racist/Sexist Tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seJEp-cxFBsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\n",
        "wordcloud = WordCloud(width=800, height=500,\n",
        "random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXMFExwTFPzo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags/trends in our twitter data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf789naaFS2T",
        "colab_type": "text"
      },
      "source": [
        "**D) Understanding the impact of Hashtags on tweets sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcGRBZT5E3Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to collect hashtags\n",
        "def hashtag_extract(x):\n",
        "    hashtags = []\n",
        "    # Loop over the words in the tweet\n",
        "    for i in x:\n",
        "        ht = re.findall(r\"#(\\w+)\", i)\n",
        "        hashtags.append(ht)\n",
        "\n",
        "    return hashtags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxRUrrZzFaZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extracting hashtags from non racist/sexist tweets\n",
        "\n",
        "HT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n",
        "\n",
        "# extracting hashtags from racist/sexist tweets\n",
        "HT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n",
        "\n",
        "# unnesting list\n",
        "HT_regular = sum(HT_regular,[])\n",
        "HT_negative = sum(HT_negative,[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQFvVxGcG0_V",
        "colab_type": "text"
      },
      "source": [
        "Now that we have prepared our lists of hashtags for both the sentiments, we can plot the top 'n' hashtags. So, first let’s check the hashtags in the non-racist/sexist tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CZebffdG4dP",
        "colab_type": "text"
      },
      "source": [
        "**Non-Racist/Sexist Tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kifsK3B0F2ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = nltk.FreqDist(HT_regular)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "\n",
        "# selecting top 20 most frequent hashtags     \n",
        "d = d.nlargest(columns=\"Count\", n = 20) \n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXYeMIckHBBI",
        "colab_type": "text"
      },
      "source": [
        "All these hashtags are positive and it makes sense. I am expecting negative terms in the plot of the second list. Let’s check the most frequent hashtags appearing in the racist/sexist tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNi8zKcwHHQg",
        "colab_type": "text"
      },
      "source": [
        "**Racist/Sexist Tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZkkZhGfG-iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = nltk.FreqDist(HT_negative)\n",
        "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
        "\n",
        "# selecting top 20 most frequent hashtags\n",
        "e = e.nlargest(columns=\"Count\", n = 20)   \n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRvgdW3YHOIN",
        "colab_type": "text"
      },
      "source": [
        "#Extracting Features from Cleaned Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJC26B9SHKhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwLRPddxHjUY",
        "colab_type": "text"
      },
      "source": [
        "##Bag-of-Words Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6YstcrEHWZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])\n",
        "bow.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrLVX-8HHt1k",
        "colab_type": "text"
      },
      "source": [
        "##TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsT5p0NJHogW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])\n",
        "tfidf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAjEysWAH3Rj",
        "colab_type": "text"
      },
      "source": [
        "##Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN4DyzWAH8mj",
        "colab_type": "text"
      },
      "source": [
        "**1. Word2Vec Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McuWgsq7HzAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing\n",
        "\n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            tokenized_tweet,\n",
        "            size=200, # desired no. of features/independent variables \n",
        "            window=5, # context window size\n",
        "            min_count=2,\n",
        "            sg = 1, # 1 for skip-gram model\n",
        "            hs = 0,\n",
        "            negative = 10, # for negative sampling\n",
        "            workers= 2, # no.of cores\n",
        "            seed = 34)\n",
        "\n",
        "model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cggXOScIQcP",
        "colab_type": "text"
      },
      "source": [
        "Let's play a bit with our Word2Vec model and see how does it perform. We will specify a word and the model will pull out the most similar words from the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDnZ66pBIE7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_w2v.wv.most_similar(positive=\"dinner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZcGKzhWIS_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_w2v.wv.most_similar(positive=\"trump\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxcWRHnKIWff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_w2v['food']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvmZrijTIZSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(model_w2v['food']) #The length of the vector is 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2y-QUUHIteq",
        "colab_type": "text"
      },
      "source": [
        "**Preparing Vectors for Tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G-mgPIgI1lg",
        "colab_type": "text"
      },
      "source": [
        "We will use the below function to create a vector for each tweet by taking the average of the vectors of the words present in the tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_Y5aCd8Ik8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0.\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model_w2v[word].reshape((1, size))\n",
        "            count += 1.\n",
        "        except KeyError: # handling the case where the token is not in vocabulary\n",
        "                         \n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU1uarV6I7TR",
        "colab_type": "text"
      },
      "source": [
        "Preparing word2vec feature set..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP0e8ZZCI4xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
        "\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
        "    \n",
        "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaDxYhATJCx5",
        "colab_type": "text"
      },
      "source": [
        "Now we have 200 new features, whereas in Bag of Words and TF-IDF we had 1000 features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja58muf-JGtp",
        "colab_type": "text"
      },
      "source": [
        "**2. Doc2Vec Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giO5Ey_8I_QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models.doc2vec import LabeledSentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHCfmXgYJR-h",
        "colab_type": "text"
      },
      "source": [
        "To implement doc2vec, we have to labelise or tag each tokenised tweet with unique IDs. We can do so by using Gensim’s LabeledSentence() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uPuQ9kkJPCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_label(twt):\n",
        "    output = []\n",
        "    for i, s in zip(twt.index, twt):\n",
        "        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91voXwDzJU_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labeled_tweets = add_label(tokenized_tweet) # label all the tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i9uNv2vJYW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labeled_tweets[:6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1--JrzNpJgxT",
        "colab_type": "text"
      },
      "source": [
        "Now let's train a doc2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIqZNzrqJbqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model \n",
        "                                  dm_mean=1, # dm = 1 for using mean of the context word vectors\n",
        "                                  size=200, # no. of desired features\n",
        "                                  window=5, # width of the context window\n",
        "                                  negative=7, # if > 0 then negative sampling will be used\n",
        "                                  min_count=5, # Ignores all words with total frequency lower than 2.\n",
        "                                  workers=3, # no. of cores\n",
        "                                  alpha=0.1, # learning rate\n",
        "                                  seed = 23)\n",
        "\n",
        "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImL9x07NJmeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_d2v.train(labeled_tweets, total_examples= len(combi['tidy_tweet']), epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GruJWyguJt1-",
        "colab_type": "text"
      },
      "source": [
        "**Preparing doc2vec Feature Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv3cIo-yJq-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
        "\n",
        "for i in range(len(combi)):\n",
        "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))\n",
        "    \n",
        "docvec_df = pd.DataFrame(docvec_arrays)\n",
        "docvec_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRRwuTZMJ0_Q",
        "colab_type": "text"
      },
      "source": [
        "#Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfzcRTdJKEIT",
        "colab_type": "text"
      },
      "source": [
        "##1. Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEkfhJjqKSs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRp38xUYKKNJ",
        "colab_type": "text"
      },
      "source": [
        "**Bag-of-Words Feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HToQpLz_JySB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bow = bow[:31962,:]\n",
        "test_bow = bow[31962:,:]\n",
        "\n",
        "# splitting data into training and validation set\n",
        "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'],  \n",
        "                                                          random_state=42, \n",
        "                                                          test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4zdUYDhKb9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lreg = LogisticRegression()\n",
        "lreg.fit(xtrain_bow, ytrain) # training the model\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
        "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "f1_score(yvalid, prediction_int) # calculating f1 score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4cREKfgLBe5",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siNbiUTKK8rV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tfidf = tfidf[:31962,:]\n",
        "test_tfidf = tfidf[31962:,:]\n",
        "\n",
        "xtrain_tfidf = train_tfidf[ytrain.index]\n",
        "xvalid_tfidf = train_tfidf[yvalid.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhrGimjjLKfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lreg.fit(xtrain_tfidf, ytrain)\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_tfidf)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBzlSFFiLS3T",
        "colab_type": "text"
      },
      "source": [
        "**Word2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjEEI1OpLNx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_w2v = wordvec_df.iloc[:31962,:]\n",
        "test_w2v = wordvec_df.iloc[31962:,:]\n",
        "\n",
        "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
        "xvalid_w2v = train_w2v.iloc[yvalid.index,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5VgGw5oLZFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lreg.fit(xtrain_w2v, ytrain)\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_w2v)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmmIfJaALeyI",
        "colab_type": "text"
      },
      "source": [
        "**Doc2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDEM4ZeqLcI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_d2v = docvec_df.iloc[:31962,:]\n",
        "test_d2v = docvec_df.iloc[31962:,:]\n",
        "\n",
        "xtrain_d2v = train_d2v.iloc[ytrain.index,:]\n",
        "xvalid_d2v = train_d2v.iloc[yvalid.index,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uH03TWUNYDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lreg.fit(xtrain_d2v, ytrain)\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_d2v)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQm1iCtvQfmE",
        "colab_type": "text"
      },
      "source": [
        "##2. Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wI8RDeNQn4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhNmCSdrQysO",
        "colab_type": "text"
      },
      "source": [
        "**Bag-of-Words Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dU2rt3JQsiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain)\n",
        "\n",
        "prediction = svc.predict_proba(xvalid_bow)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wghD35ERUgP",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-lb26dMRKe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_tfidf, ytrain)\n",
        "\n",
        "prediction = svc.predict_proba(xvalid_tfidf)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njqErJxORqV_",
        "colab_type": "text"
      },
      "source": [
        "**Word2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hww9By1eRpgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_w2v, ytrain)\n",
        "\n",
        "prediction = svc.predict_proba(xvalid_w2v)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvOcAVrSRcbS",
        "colab_type": "text"
      },
      "source": [
        "**Doc2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaTsxB29Ra7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_d2v, ytrain)\n",
        "\n",
        "prediction = svc.predict_proba(xvalid_d2v)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSPdVAWrR22q",
        "colab_type": "text"
      },
      "source": [
        "##3. RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v54rsRhYR172",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7O68H7WSEuj",
        "colab_type": "text"
      },
      "source": [
        "**Bag-of-Words Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnBp8LM7RjyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain)\n",
        "\n",
        "prediction = rf.predict(xvalid_bow)\n",
        "f1_score(yvalid, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxP8acvOSQrh",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N83SxumYSPHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain)\n",
        "\n",
        "prediction = rf.predict(xvalid_tfidf)\n",
        "f1_score(yvalid, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5aAAq-mSsyE",
        "colab_type": "text"
      },
      "source": [
        "**Word2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1RTP3j-SY6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain)\n",
        "\n",
        "prediction = rf.predict(xvalid_w2v)\n",
        "f1_score(yvalid, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzzE-wErSiOa",
        "colab_type": "text"
      },
      "source": [
        "**Doc2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO-OyK_BSnrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_d2v, ytrain)\n",
        "\n",
        "prediction = rf.predict(xvalid_d2v)\n",
        "f1_score(yvalid, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVI4Aa7vSzUX",
        "colab_type": "text"
      },
      "source": [
        "##4. XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIrbpASZSrnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyHdnpZpTD9c",
        "colab_type": "text"
      },
      "source": [
        "**Bag-of-Words Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfvVFu1GS61m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)\n",
        "prediction = xgb_model.predict(xvalid_bow)\n",
        "f1_score(yvalid, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gvcjJpVTRJX",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8d9-cU5TP_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain)\n",
        "\n",
        "prediction = xgb.predict(xvalid_tfidf)\n",
        "f1_score(yvalid, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slX7kkHXTZcx",
        "colab_type": "text"
      },
      "source": [
        "**Word2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmAbvMy0TYDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain)\n",
        "\n",
        "prediction = xgb.predict(xvalid_w2v)\n",
        "f1_score(yvalid, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2hg9uMkT9bZ",
        "colab_type": "text"
      },
      "source": [
        "**Doc2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPkdImEmUDJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_d2v, ytrain)\n",
        "\n",
        "prediction = xgb.predict(xvalid_d2v)\n",
        "f1_score(yvalid, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvDv8-XMTtxH",
        "colab_type": "text"
      },
      "source": [
        "***Note***: XGBoost model on word2vec features has outperformed all the previuos models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ti7N3gPUX1l",
        "colab_type": "text"
      },
      "source": [
        "#Model Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkORWyAAVLcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekNZmd2sVNsG",
        "colab_type": "text"
      },
      "source": [
        "Here we are using DMatrices. A DMatrix can contain both the features and the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwD5SMo2VLhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain)\n",
        "dvalid = xgb.DMatrix(xvalid_w2v, label=yvalid)\n",
        "dtest = xgb.DMatrix(test_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvc-qOeKVXIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters that we are going to tune\n",
        "params = {\n",
        "    'objective':'binary:logistic',\n",
        "    'max_depth':6,\n",
        "    'min_child_weight': 1,\n",
        "    'eta':.3,\n",
        "    'subsample': 1,\n",
        "    'colsample_bytree': 1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xn_rAl3VhE_",
        "colab_type": "text"
      },
      "source": [
        "We will prepare a custom evaluation metric to calculate F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBsMfXVAVduk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_eval(preds, dtrain):\n",
        "    labels = dtrain.get_label().astype(np.int)\n",
        "    preds = (preds >= 0.3).astype(np.int)\n",
        "    return [('f1_score', f1_score(labels, preds))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5FJWuDSV08P",
        "colab_type": "text"
      },
      "source": [
        "**General Approach for Parameter Tuning**\n",
        "\n",
        "We will follow the steps below to tune the parameters.\n",
        "\n",
        "<ol>\n",
        "<li>Choose a relatively high learning rate. Usually a learning rate of 0.3 is used at this stage.\n",
        "<li>Tune tree-specific parameters such as max_depth, min_child_weight, subsample, colsample_bytree keeping the learning rate fixed.\n",
        "<li>Tune the learning rate.\n",
        "<li>Finally tune gamma to avoid overfitting.\n",
        "</ol>\n",
        "Tuning max_depth and min_child_weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K7D_Y9YVmOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gridsearch_params = [\n",
        "    (max_depth, min_child_weight)\n",
        "    for max_depth in range(6,10) \n",
        "    for min_child_weight in range(5,8)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-TKc_K9WzUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_f1 = 0. # initializing with 0\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "\n",
        "    # Update our parameters\n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        feval= custom_eval,\n",
        "        num_boost_round=200,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "\n",
        "    # Finding best F1 Score\n",
        "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
        "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "    if mean_f1 > max_f1:\n",
        "        max_f1 = mean_f1\n",
        "        best_params = (max_depth,min_child_weight)\n",
        "\n",
        "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIk90PwoW3Sp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Updating max_depth and min_child_weight parameters.\n",
        "params['max_depth'] = 7\n",
        "params['min_child_weight'] = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1cHpIpKXBXy",
        "colab_type": "text"
      },
      "source": [
        "Tuning subsample and colsample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr9XJBRYW92Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gridsearch_params = [\n",
        "    (subsample, colsample)\n",
        "    for subsample in [i/10. for i in range(5,10)]\n",
        "    for colsample in [i/10. for i in range(5,10)]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzJa8Wy7XFiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_f1 = 0.\n",
        "best_params = None\n",
        "for subsample, colsample in gridsearch_params:\n",
        "    print(\"CV with subsample={}, colsample={}\".format(\n",
        "                             subsample,\n",
        "                             colsample))\n",
        "\n",
        "    # Update our parameters\n",
        "    params['colsample'] = colsample\n",
        "    params['subsample'] = subsample\n",
        "\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        feval= custom_eval,\n",
        "        num_boost_round=200,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "\n",
        "    # Finding best F1 Score\n",
        "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
        "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "    if mean_f1 > max_f1:\n",
        "        max_f1 = mean_f1\n",
        "        best_params = (subsample, colsample)\n",
        "\n",
        "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HW6cUcrXLK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Updating subsample and colsample_bytree.\n",
        "params['subsample'] = .9\n",
        "params['colsample_bytree'] = .5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eYHQB7tXUbr",
        "colab_type": "text"
      },
      "source": [
        "Now let's tune the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpD6e_hZXSN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_f1 = 0.\n",
        "best_params = None\n",
        "for eta in [.3, .2, .1, .05, .01, .005]:\n",
        "    print(\"CV with eta={}\".format(eta))\n",
        "\n",
        "    # Update ETA\n",
        "    params['eta'] = eta\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        feval= custom_eval,\n",
        "        num_boost_round=1000,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=20\n",
        "    )\n",
        "\n",
        "    # Finding best F1 Score\n",
        "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
        "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "    if mean_f1 > max_f1:\n",
        "        max_f1 = mean_f1\n",
        "        best_params = eta\n",
        "\n",
        "print(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpcx-0NHXbVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Updating learning rate\n",
        "params['eta'] = .1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYmizG6JXqo1",
        "colab_type": "text"
      },
      "source": [
        "Now lets tune gamma value using the parameters already tuned above. We’ll check for 5 values here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VolV-n8rXp7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_f1 = 0.\n",
        "best_params = None\n",
        "for gamma in range(0,15):\n",
        "    print(\"CV with gamma={}\".format(gamma/10.))\n",
        "\n",
        "    # Update ETA\n",
        "    params['gamma'] = gamma/10.\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        feval= custom_eval,\n",
        "        num_boost_round=200,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "\n",
        "    # Finding best F1 Score\n",
        "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
        "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "    if mean_f1 > max_f1:\n",
        "        max_f1 = mean_f1\n",
        "        best_params = gamma/10.\n",
        "\n",
        "print(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umRgAXnmXnab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Updating gamma value\n",
        "params['gamma'] = 1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i99gKujKX9qt",
        "colab_type": "text"
      },
      "source": [
        "Let's have a look at the final list of parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM2jRiAwX5tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP70gigPYCHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    feval= custom_eval,\n",
        "    num_boost_round= 1000,\n",
        "    maximize=True,\n",
        "    evals=[(dvalid, \"Validation\")],\n",
        "    early_stopping_rounds=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7P8_dfcYFd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = xgb_model.predict(dtest)\n",
        "test['label'] = (test_pred >= 0.3).astype(np.int)\n",
        "submission = test[['id','label']]\n",
        "submission.to_csv('sub_xgb_w2v_06062018.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mDgVqIzYKkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}